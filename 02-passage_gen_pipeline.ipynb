{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b098af9-9f80-45c2-89a5-4270676e6bee",
   "metadata": {},
   "source": [
    "# Operationalize product data and content enrichment using GenAI and AutoSxS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f72c331-725b-4937-9852-f01992fac185",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This series of notebooks showcases an end-to-end workflow for improving product catalog data using Generative AI and MLOps. The core focus is to operationalize the process for enriching product descriptions, a key element for effective product discovery and recommendation systems.\n",
    "\n",
    "Building upon the previous notebook's work, where we enhanced descriptions using a GenAI model (Gemma), we now compare these kinds of responses with those from Google's text-bison models using an automated evaluation system.\n",
    "\n",
    "In this notebook, downstream processes such as model tuning and logging results to data repositories are triggered based on these types of evaluations using AutoSxS. This work flow is then orchestrated and managed efficiently within Vertex AI Pipelines, ensuring a robust and scalable solution for continuous product data improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49beb9ed-9cc2-4a11-b22c-a9762204f4f3",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "* Parameters, variables, and helper functions are defined\n",
    "* A base Docker image containing dependencies for pipeline components is created and stored in Artifact Registry for future use\n",
    "* Component Definition:\n",
    "    * A component for generating product descriptions using the chosen GenAI model is defined.\n",
    "    * An AutoSxS component is defined to evaluate the quality of generated descriptions.\n",
    "    * Conditional downstream tasks, such as model retuning or data logging, are defined as separate components based on evaluation results.\n",
    "* Components are integrated into a Vertex AI Pipeline, which is then submitted as a pipeline job to automate and manage the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175194e4-a58f-47ed-b4ef-73e52660bc90",
   "metadata": {},
   "source": [
    "### (prereq) Dataset Information\n",
    "\n",
    "The 'Input Feed' data is based on a collection of 1,000-row random sample of data from the public BigQuery dataset 'theLook eCommerce'. The data including text and enriched attributes was generated by [FeedGen](https://github.com/google-marketing-solutions/feedgen) and was extracted as a CSV from the FeedGen [input feed - template sheet](https://docs.google.com/spreadsheets/d/19eKTJrbZaUfipAvL5ZQmq_hoxEbLQIlDqURKFJA2OBU/edit#gid=1661242997)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd7b5c6-2610-4528-9ebd-dea57af7b561",
   "metadata": {},
   "source": [
    "## (Optional) Create endpoint of incumbent model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef501a4-11d5-4395-8abb-45811afb6bce",
   "metadata": {},
   "source": [
    "## Install additional packages\n",
    "Install the following packages required to execute this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0699e3c-514e-4be1-9ff7-b0524000b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install the packages\n",
    "# ! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n",
    "#                                  google-cloud-storage \\\n",
    "#                                  kfp \\\n",
    "#                                  google-cloud-pipeline-components \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b716fcf5-dd3b-4190-98b8-d44d1f48e3bb",
   "metadata": {},
   "source": [
    "## Import Libraries & Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a46cf4f2-ba98-480b-a4af-838508cf9cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
      "  return component_factory.create_component_from_func(\n",
      "/var/tmp/ipykernel_19877/1800243730.py:21: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
      "  from kfp.v2 import compiler\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "\n",
    "from google.cloud import aiplatform, language\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google_cloud_pipeline_components.preview import model_evaluation\n",
    "from kfp import compiler\n",
    "\n",
    "import vertexai\n",
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (\n",
    "    component,\n",
    "    pipeline,\n",
    "    Input,\n",
    "    Output,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8b7465-acf0-4257-9c7c-cf4f9f7668f4",
   "metadata": {},
   "source": [
    "### Set Parameters and Variables\n",
    "\n",
    "\n",
    "* `PROJECT_ID`: The ID of your Google Cloud project where the pipeline and resources reside\n",
    "* `REGION`: The Google Cloud region where pipeline components are executed and resources are located\n",
    "* `ARTIFACT_REPO`: The name of the repository used for storing pipeline artifacts and container images in Artifact Registry\n",
    "* `BUCKET_URI`: The URI of the Google Cloud Storage bucket where data and pipeline artifacts are stored (e.g., \"gs://passage-gen-test\").\n",
    "* `BUCKET_NAME`: The name of the GCS bucket derived from the BUCKET_URI\n",
    "* `MODEL_RESOURCE`: The resource name of the baseline language model used for comparison in AutoSxS (e.g., \"publishers/google/models/text-bison-32k@002\").\n",
    "* `IMAGE_URI`: The URI of the container image used for running the pipeline components. It's dynamically constructed using the REGION, PROJECT_ID, and ARTIFACT_REPO.\n",
    "* `input_feed_data`: The GCS path to the input CSV file containing product data for generating descriptions\n",
    "* `evaluation_dataset_name`: The base name for the generated evaluation dataset files (without extension), used for both CSV and JSONL formats.\n",
    "* `judgement_threshold`: The minimum win rate required for either model (custom or baseline) to be considered for updating the product catalog (e.g., 0.75).\n",
    "* `SERVICE_ACCOUNT`: The service account used for running the pipeline components and accessing Google Cloud resources.\n",
    "* `input_file_path`: (Appears to be the same as input_feed_data) The GCS path to the input CSV file containing product data.\n",
    "* `PIPELINE_ROOT`: The GCS location where pipeline metadata and execution information are stored. It's set to the same value as BUCKET_URI.\n",
    "* `DISPLAY_NAME`: The display name for the pipeline run, constructed using the ARTIFACT_REPO and a random string.\n",
    "* `DATASET_ID`: The ID of the BigQuery dataset where evaluation results or other data might be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8c4021e-f01f-4c18-b9eb-698b87f7d99d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"sandbox-401718\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "ARTIFACT_REPO = \"passage-gen-example\"\n",
    "\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}-passage-gen-test\"  # @param {type:\"string\"}\n",
    "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "MODEL_RESOURCE = \"publishers/google/models/text-bison-32k@002\"\n",
    "# MODEL_RESOURCE = \"publishers/google/models/gemini-1.0-pro-001\"\n",
    "random_str = \"\".join(random.choices(string.ascii_lowercase + string.digits, k=5))\n",
    "IMAGE_URI=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{ARTIFACT_REPO}/passage_gen_image:latest\"\n",
    "\n",
    "input_feed_data = \"gs://passage-gen-test/FeedGen-Input-Feed.csv\"  # INPUT DATA GCS path\n",
    "evaluation_dataset_name = \"evaluation_dataset_pipe\"\n",
    "# endpoint_resource_name = \"projects/757654702990/locations/us-central1/endpoints/2501228424492744704\" # if not specified textbison001 will be used\n",
    "\n",
    "judgement_threshold = 0.75\n",
    "\n",
    "SERVICE_ACCOUNT = (\n",
    "    \"757654702990-compute@developer.gserviceaccount.com\"  # @param {type:\"string\"}\n",
    ")\n",
    "\n",
    "PIPELINE_ROOT = BUCKET_URI\n",
    "DISPLAY_NAME = ARTIFACT_REPO + random_str\n",
    "DATASET_ID = \"passage_gen_autosxs\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f5a389-fcfc-4471-a657-9302c84d6535",
   "metadata": {},
   "source": [
    "**Only if your bucket doesn't already exist:** Run the following cell to create your Cloud Storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b51150-7cec-4c9e-b3a8-eb90b0f3a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb2e2e-fecb-462f-8e9d-bbe255592cab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Python File with Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10c90ec6-b641-4b5b-a6b1-3cface1dd533",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./utils.py\n",
    "    \n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "\n",
    "from google.cloud import aiplatform, language\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google_cloud_pipeline_components.preview import model_evaluation\n",
    "from kfp import compiler\n",
    "\n",
    "import vertexai\n",
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "def prompt_func(prompt_input: str):\n",
    "    \"\"\"Prompts designed to enrich Product description information\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "        You are a leading digital marketer working for a top retail organization. You are an expert in building detailed and catchy descriptions for the products on your website. \n",
    "\n",
    "        Context: {prompt_input}\n",
    "\n",
    "        Generate ONLY the product description in English that highlights the product's features using the above \"Context\" information. \n",
    "        If you find a \"description\" in the given \"Context\", do NOT reuse it, but make sure you describe any features listed within it in more detail. \n",
    "        Do NOT repeat sentences. The generated description should strictly be about the provided product. \n",
    "        Correct product type, number of items contained in the the product as well as product features such as color should be followed. \n",
    "        Any product features that are not present in the input should not be present in the generated description.\n",
    "        Hyperbolic text, over promising or guarantees are to be avoided.\n",
    "        The generated description should be at least 50 words long, preferably at least 150. \n",
    "        The generated description MUST NOT use special characters or any Markdown or JSON syntax. \n",
    "\n",
    "        New Detailed Product Description:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    \"\"\"Generates a custom-length UUID-like string.\n",
    "\n",
    "    Uses a combination of lowercase letters and digits to create a \n",
    "    randomized string resembling a shortened UUID (Universally Unique Identifier).\n",
    "\n",
    "    Args:\n",
    "        length (int, optional): The desired length of the generated string. \n",
    "                                Defaults to 8 characters.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated UUID-like string.\n",
    "    \"\"\"\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "UUID = generate_uuid()\n",
    "\n",
    "\n",
    "# Gemma deployment\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    \"\"\"Gets the job name with date time when triggering deployment jobs.\"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"g2-standard-12\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    max_model_len: int = 8192,\n",
    "    dtype: str = \"bfloat16\",\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys models with vLLM on GPU in Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        \"--gpu-memory-utilization=0.9\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "\n",
    "    env_vars = {\n",
    "        \"MODEL_ID\": model_id,\n",
    "    }\n",
    "    # if HF_TOKEN:\n",
    "    #     env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "        serving_container_deployment_timeout=7200,\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "        sync=True,\n",
    "        enable_access_logging=True,\n",
    "    )\n",
    "    return model, endpoint\n",
    "\n",
    "\n",
    "def save_csv_gcs(BUCKET_NAME: str, evaluation_dataset_name: str):\n",
    "    \"\"\"\n",
    "    Saves a CSV file to a Google Cloud Storage bucket.\n",
    "\n",
    "    Args:\n",
    "        BUCKET_NAME (str):  The name of the GCS bucket (excluding the 'gs://' prefix).\n",
    "        evaluation_dataset_name (str):  The filename (without extension) to use for the saved CSV.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # save to GCS\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(BUCKET_NAME[5:])\n",
    "    blob = bucket.blob(f\"data/{evaluation_dataset_name}.csv\")\n",
    "    blob.upload_from_filename(f\"{evaluation_dataset_name}.csv\")\n",
    "\n",
    "    print(f\"File uploaded to cloud storage in {BUCKET_NAME}/data/{evaluation_dataset_name}.csv\")\n",
    "    \n",
    "    \n",
    "    \n",
    "def save_jsonl_gcs(BUCKET_NAME: str, evaluation_dataset_name: str):\n",
    "    \"\"\"\n",
    "    Saves a JSON Lines (.jsonl) file to a Google Cloud Storage bucket.\n",
    "\n",
    "    Args:\n",
    "        BUCKET_NAME (str):  The name of the GCS bucket (excluding the 'gs://' prefix).\n",
    "        evaluation_dataset_name (str):  The filename (without extension) to use for the saved JSONL file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # save to GCS \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(BUCKET_NAME[5:])\n",
    "    blob = bucket.blob(f\"data/{evaluation_dataset_name}.jsonl\")\n",
    "    blob.upload_from_filename(f\"{evaluation_dataset_name}.jsonl\")\n",
    "\n",
    "    print(f\"File uploaded to cloud storage in {BUCKET_NAME}/data/{evaluation_dataset_name}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b8e8c5-5a54-427c-91e4-07574f07d2b6",
   "metadata": {},
   "source": [
    "## Create Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75092268-7cd2-4d81-9785-69b2c829c7da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM python:3.10-slim\n",
    "\n",
    "COPY ./utils.py ./utils.py\n",
    "COPY ./requirements.txt ./requirements.txt\n",
    "\n",
    "WORKDIR ./app\n",
    "RUN apt-get update && apt-get install gcc libffi-dev -y\n",
    "\n",
    "RUN pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea6a4101-d48e-410f-ba45-c44df598dbd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  6.348MB\n",
      "Step 1/6 : FROM python:3.10-slim\n",
      " ---> 797a4d7093b1\n",
      "Step 2/6 : COPY ./utils.py ./utils.py\n",
      " ---> Using cache\n",
      " ---> ccde40f93680\n",
      "Step 3/6 : COPY ./requirements.txt ./requirements.txt\n",
      " ---> Using cache\n",
      " ---> 44247a683f3e\n",
      "Step 4/6 : WORKDIR ./app\n",
      " ---> Using cache\n",
      " ---> 42707595c675\n",
      "Step 5/6 : RUN apt-get update && apt-get install gcc libffi-dev -y\n",
      " ---> Using cache\n",
      " ---> a482880a5ff1\n",
      "Step 6/6 : RUN pip install -r requirements.txt\n",
      " ---> Running in b14351fd55f0\n",
      "\u001b[91mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mThe command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 1\n",
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n",
      "The push refers to repository [us-central1-docker.pkg.dev/sandbox-401718/passage-gen-example/passage_gen_image]\n",
      "\n",
      "\u001b[1Baf07e5a8: Preparing \n",
      "\u001b[1B2efef37c: Preparing \n",
      "\u001b[1B43e99092: Preparing \n",
      "\u001b[1B252dc60c: Preparing \n",
      "\u001b[1B81090b57: Preparing \n",
      "\u001b[1B72ec4bd1: Preparing \n",
      "\u001b[1B86592bf9: Preparing \n",
      "\u001b[1B081d1eb2: Preparing \n",
      "\u001b[2B081d1eb2: Layer already exists \u001b[3A\u001b[2K\u001b[2A\u001b[2Klatest: digest: sha256:b86d8dfeacdff4357c10bdb4563ed5160128f9c5ea69846de8766614106f0c75 size: 2210\n"
     ]
    }
   ],
   "source": [
    "! docker build -t {IMAGE_URI} .\n",
    "! gcloud auth configure-docker us-central1-docker.pkg.dev --quiet\n",
    "! docker push {IMAGE_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c930f798-75dd-4ac2-90e5-ad683458b20c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Components\n",
    "\n",
    "Components are building blocks for creating kfp pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e53a9d-4f6f-43a3-b23f-523cbfdbaee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=IMAGE_URI,\n",
    "    packages_to_install=[\"google-cloud-aiplatform\",\"google-cloud-language\", \"fsspec\", \"gcsfs\"],\n",
    ")\n",
    "def gen_model_endpoint_response(\n",
    "    BUCKET_NAME: str,\n",
    "    input_feed_data: str,\n",
    "    evaluation_dataset_name: str,\n",
    "    max_tokens: int = 1000,\n",
    "    temperature: float = 0.5,\n",
    "    top_p: float = 0.5,\n",
    "    top_k: int = 10,\n",
    "    endpoint_resource_name: str = None,\n",
    "):\n",
    "\n",
    "    \"\"\"Generates enriched product descriptions using a language model and saves results.\n",
    "\n",
    "    This component leverages a language model to create detailed, marketing-oriented \n",
    "    product descriptions based on input product data. It takes a CSV file and \n",
    "    output dataset name, processes the product information, prompts a language model,\n",
    "    and saves the generated descriptions along with relevant metadata.\n",
    "\n",
    "    Args:\n",
    "        input_feed_data (str): Path to the CSV file containing product data \n",
    "                             (columns like 'Title', 'Item ID', etc.).\n",
    "        evaluation_dataset_name (str): Name for the dataset of results, used when\n",
    "                                     saving to cloud storage.\n",
    "        max_tokens (int):  Maximum number of tokens in generated responses (default: 1000).  \n",
    "        temperature (float): Controls the randomness of generated text (default: 0.5).\n",
    "        top_p (float): Nucleus sampling parameter (default: 0.5).\n",
    "        top_k (int): Number of tokens to consider for nucleus sampling (default: 10).\n",
    "        endpoint_resource_name (str): Resource name of a custom language model endpoint. If not provided, a default base model is used.\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import aiplatform, language\n",
    "    import random\n",
    "    import utils\n",
    "    import vertexai\n",
    "    from vertexai.preview.language_models import TextGenerationModel\n",
    "    \n",
    "    # Import input\n",
    "    df = pd.read_csv(input_feed_data) \\\n",
    "      .drop(['Link', 'Image Link'], axis=1) \\\n",
    "      .head(5)\n",
    "    \n",
    "    eval_df = []\n",
    "\n",
    "    # Generate description for each product row\n",
    "    for index, row in df.iterrows():\n",
    "        if index % 5 == 0:\n",
    "            print(\"Processing row:\", index+1)\n",
    "\n",
    "        # prompt_input = row.result\n",
    "        prompt_input = row.to_dict()\n",
    "        prompt = utils.prompt_func(prompt_input)\n",
    "        instances = [\n",
    "            {\n",
    "                \"prompt\": prompt,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": top_p,\n",
    "                \"top_k\": top_k,\n",
    "                \"raw_response\": True,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # try: \n",
    "        if endpoint_resource_name is not None:\n",
    "            endpoint_vllm = aiplatform.Endpoint(endpoint_resource_name)\n",
    "            response = endpoint_vllm.predict(instances=instances)\n",
    "            prediction = response.predictions[0]\n",
    "        # except:\n",
    "        elif endpoint_resource_name is None:\n",
    "            model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "            prediction = model.predict(\n",
    "                instances[0][\"prompt\"],\n",
    "                temperature=instances[0][\"temperature\"],\n",
    "                max_output_tokens=instances[0][\"max_tokens\"],\n",
    "                top_k=instances[0][\"top_k\"],\n",
    "                top_p=instances[0][\"top_p\"],\n",
    "            ).text\n",
    "\n",
    "        # Append results\n",
    "        eval_df.append(\n",
    "            {\n",
    "                \"prompt_id\": prompt_input,\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": prediction,\n",
    "                \"name\": prompt_input[\"Title\"],\n",
    "                \"id\": prompt_input[\"Item ID\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        eval_df_ = pd.DataFrame(eval_df)\n",
    "\n",
    "    # save locally\n",
    "    eval_df_.to_csv(f\"{evaluation_dataset_name}.csv\", index=False)\n",
    "\n",
    "    # save to GCS\n",
    "    utils.save_csv_gcs(BUCKET_NAME, evaluation_dataset_name)\n",
    "\n",
    "    eval_df_.to_json(f\"{evaluation_dataset_name}.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "    # save to GCS\n",
    "    utils.save_jsonl_gcs(BUCKET_NAME, evaluation_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae238c7-48d8-442a-a1b8-15c7a970d242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=IMAGE_URI,\n",
    "    packages_to_install=[\"google-cloud-aiplatform\",\"google-cloud-language\", \"fsspec\", \"gcsfs\"],\n",
    ")\n",
    "def autoSxS_compare_base_model(\n",
    "    BUCKET_NAME: str,\n",
    "    evaluation_dataset_name: str,\n",
    "    PROJECT_ID: str,\n",
    "    REGION: str,\n",
    "    BUCKET_URI: str,\n",
    "    # MODEL_RESOURCE: str = \"publishers/google/models/text-bison-32k@002\",\n",
    "    MODEL_RESOURCE: str = \"publishers/google/models/gemini-1.0-pro-001\",\n",
    "    context_column: str = \"name\",\n",
    "    question_column: str = \"prompt\",\n",
    "    response_column: str = \"response\",\n",
    "    model_prompt: str = \"prompt\",\n",
    ") -> NamedTuple(\n",
    "    \"Outputs\", [(\"judgements_path\", str), (\"win_rate_a\", float), (\"win_rate_b\", float)]\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes an AutoSxS model evaluation job on Google Cloud AI Platform, comparing \n",
    "    responses to a baseline model.\n",
    "\n",
    "    This component performs the following: Configures AutoSxS Job. Initializes an AutoSxS Pipeline Job\n",
    "    Launches the Job. Submits the Pipeline Job to AI Platform and waits for completion.\n",
    "    Extracts Results and logs outputs.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        BUCKET_NAME (str): Name of the Google Cloud Storage bucket.\n",
    "        evaluation_dataset_name (str): Name of the evaluation dataset (a JSONL file in GCS).\n",
    "        PROJECT_ID (str): Project ID for the Google Cloud project.\n",
    "        REGION (str): Region where the AutoSxS job will run.\n",
    "        BUCKET_URI (str):  URI of a GCS bucket for staging pipeline artifacts.\n",
    "        MODEL_RESOURCE (str): Resource name of the baseline model (default: 'publishers/google/models/text-bison-32k@002')\n",
    "        context_column (str): Column in the dataset containing contextual information for prompts.\n",
    "        question_column (str):  Column in the dataset containing the prompts.\n",
    "        response_column (str): Name of the column where the custom model's responses will be stored.\n",
    "        model_prompt (str): Prompt used with the custom model.\n",
    "\n",
    "    Returns:\n",
    "        NamedTuple('Outputs'):\n",
    "            judgements_path (str): Path to the judgements.jsonl file (GCS location).\n",
    "            win_rate_a (float): Win rate of the custom model.\n",
    "            win_rate_b (float): Win rate of the baseline model.\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import aiplatform, language\n",
    "    import random\n",
    "    from typing import NamedTuple\n",
    "    import utils\n",
    "\n",
    "    UUID = utils.generate_uuid()\n",
    "    display_name = f\"examples-resp-model-full-32k-{UUID}\"\n",
    "    context_column = context_column\n",
    "    question_column = question_column\n",
    "    response_column = response_column\n",
    "    model_prompt = model_prompt\n",
    "    model_resource = MODEL_RESOURCE\n",
    "\n",
    "    parameters = {\n",
    "        \"evaluation_dataset\": f\"{BUCKET_NAME}/data/{evaluation_dataset_name}.jsonl\",\n",
    "        \"id_columns\": [question_column],\n",
    "        \"autorater_prompt_parameters\": {\n",
    "            \"inference_context\": {\"column\": context_column},\n",
    "            \"inference_instruction\": {\"column\": question_column},\n",
    "        },\n",
    "        \"task\": \"question_answering@001\",\n",
    "        \"model_a\": model_resource,\n",
    "        \"model_a_prompt_parameters\": {\"prompt\": {\"column\": model_prompt}},\n",
    "        \"response_column_b\": response_column,\n",
    "    }\n",
    "\n",
    "    aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
    "    job = aiplatform.PipelineJob(\n",
    "        job_id=display_name,\n",
    "        display_name=display_name,\n",
    "        pipeline_root=os.path.join(BUCKET_URI, display_name),\n",
    "        # template_path=template_uri,\n",
    "        template_path=(\n",
    "            \"https://us-kfp.pkg.dev/ml-pipeline/google-cloud-registry/autosxs-template/default\"\n",
    "        ),\n",
    "        parameter_values=parameters,\n",
    "        enable_caching=False,\n",
    "    )\n",
    "    job.run(sync=True)\n",
    "    print(\"Pipeline Complete\")\n",
    "\n",
    "    # Aggregate Metrics\n",
    "\n",
    "    for details in job.task_details:\n",
    "        if details.task_name == \"model-evaluation-text-generation-pairwise\":\n",
    "            break\n",
    "    # Extract judgements output path\n",
    "\n",
    "    string = str(details.__dict__)\n",
    "\n",
    "    items = string.strip(\"{}\").split(\"\\n\")\n",
    "    for item in items:\n",
    "        if \"judgments.jsonl\" in item:\n",
    "            _, value = item.split('string_value: \"')\n",
    "            judgements_path = value.strip('\"')\n",
    "    # Extract win rates\n",
    "\n",
    "    win_rate = pd.DataFrame([details.outputs[\"autosxs_metrics\"].artifacts[0].metadata])\n",
    "    win_rate_a = win_rate[\"autosxs_model_a_win_rate\"][0]\n",
    "    win_rate_b = win_rate[\"autosxs_model_b_win_rate\"][0]\n",
    "\n",
    "    return (judgements_path, win_rate_a, win_rate_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a9a02c7-12aa-4f6d-a3aa-e85c940da7e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=IMAGE_URI,\n",
    "    packages_to_install=[\"google-cloud-aiplatform\",\"google-cloud-language\", \"fsspec\", \"gcsfs\"],\n",
    ")\n",
    "def update_catalog(\n",
    "    BUCKET_NAME: str,\n",
    "    evaluation_dataset_name: str,\n",
    "    judgements_uri: str,\n",
    "    win_rate_a: float,\n",
    "    win_rate_b: float,\n",
    "    judgement_threshold: float,\n",
    "):\n",
    "    \"\"\"\n",
    "    Updates a catalog based on evaluation results and a win-rate threshold.\n",
    "\n",
    "    Reads evaluation results and judgements from specified sources, determines \n",
    "    the winning model (if any), and saves the winning responses to GCS.\n",
    "\n",
    "    Args:\n",
    "        BUCKET_NAME (str): The name of the GCS bucket (excluding 'gs://' prefix).\n",
    "        evaluation_dataset_name (str): Filename of the evaluation dataset (without extension).\n",
    "        judgements_uri (str): URI of the judgements file (likely a GCS object path).\n",
    "        judgement_threshold (float):  Win-rate threshold for determining the winning model (default=0.75).\n",
    "        win_rate_a (float): Win rate of model A.\n",
    "        win_rate_b (float): Win rate of model B.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    import utils\n",
    "\n",
    "    # Fetch Judgements from AutoSxS job\n",
    "    judgements_df = pd.read_json(judgements_uri, lines=True)\n",
    "    eval_df_ = pd.read_csv(f\"{BUCKET_NAME}/data/{evaluation_dataset_name}.csv\")\n",
    "    response_df = pd.merge(\n",
    "        eval_df_[[\"prompt\", \"id\"]], judgements_df, on=\"prompt\", how=\"left\"\n",
    "    ).dropna()\n",
    "\n",
    "    # Evaluate if win rate meets threshold\n",
    "\n",
    "    if win_rate_a >= judgement_threshold:\n",
    "        drop_column = \"response_b\"\n",
    "    elif win_rate_b >= judgement_threshold:\n",
    "        drop_column = \"response_a\"\n",
    "        \n",
    "    # Write winner to GCS\n",
    "\n",
    "    response_df = response_df.drop(\n",
    "        [\"explanation\", \"choice\", \"confidence\", drop_column], axis=1\n",
    "    )\n",
    "    response_df.to_json(\n",
    "        f\"{evaluation_dataset_name}_win_responses.jsonl\", orient=\"records\", lines=True\n",
    "    )\n",
    "    utils.save_jsonl_gcs(BUCKET_NAME, f\"{evaluation_dataset_name}_win_responses\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d02378f-3157-4900-84f8-86ac889e5912",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"us-central1-docker.pkg.dev/sandbox-401718/passage-gen-example/passage_gen_image:latest\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\",\"google-cloud-language\", \"fsspec\", \"gcsfs\"],\n",
    ")\n",
    "def hello_world():\n",
    "    print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf2809e-5ff6-4b64-ac1a-5f9fe8b3593e",
   "metadata": {},
   "source": [
    "## Define and Run Pipeline\n",
    "\n",
    "This Kubeflow pipeline automates product description enrichment and evaluation. This leverages a language model in Vertex AI to generate detailed descriptions based on input product data. Then, it employs AutoSxS to compare these descriptions against a baseline model.\n",
    "\n",
    "* A component for generating product descriptions using the chosen GenAI model is defined.\n",
    "* An AutoSxS component is defined to evaluate the quality of generated descriptions.\n",
    "* Conditional downstream tasks, such as model tuning or data logging, are defined as separate components based on evaluation results.\n",
    "\n",
    "Reference notebook for fine tuning Gemma from the Google Cloud repository: [model_garden_gemma_finetuning_on_vertex.ipynb](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_finetuning_on_vertex.ipynb)\n",
    "\n",
    "<br>\n",
    "\n",
    "![pipeline.png](./imgs/passage-gen-pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d277d43e-4962-4cc5-b2dc-ff53d2b057d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(pipeline_root=PIPELINE_ROOT, name=\"passage-gen-example\")\n",
    "def pipeline():\n",
    "\n",
    "    generate_responses = gen_model_endpoint_response(\n",
    "        BUCKET_NAME=BUCKET_NAME,\n",
    "        input_feed_data=input_feed_data,\n",
    "        evaluation_dataset_name=evaluation_dataset_name,\n",
    "    )\n",
    "\n",
    "    autosxs_evals = autoSxS_compare_base_model(\n",
    "        BUCKET_NAME=BUCKET_NAME,\n",
    "        MODEL_RESOURCE=MODEL_RESOURCE,\n",
    "        evaluation_dataset_name=evaluation_dataset_name,\n",
    "        PROJECT_ID=PROJECT_ID,\n",
    "        REGION=REGION,\n",
    "        BUCKET_URI=BUCKET_URI,\n",
    "        context_column=\"name\",\n",
    "        question_column=\"prompt\",\n",
    "        response_column=\"response\",\n",
    "        model_prompt=\"prompt\",\n",
    "    ).after(generate_responses)\n",
    "\n",
    "    with dsl.If(\n",
    "        (autosxs_evals.outputs[\"win_rate_a\"] >= judgement_threshold)\n",
    "        or (autosxs_evals.outputs[\"win_rate_b\"] >= judgement_threshold)\n",
    "    ):\n",
    "        update_catalog(\n",
    "            BUCKET_NAME=BUCKET_NAME,\n",
    "            evaluation_dataset_name=evaluation_dataset_name,\n",
    "            judgements_uri=autosxs_evals.outputs[\"judgements_path\"],\n",
    "            judgement_threshold=judgement_threshold,\n",
    "            win_rate_a=autosxs_evals.outputs[\"win_rate_a\"],\n",
    "            win_rate_b=autosxs_evals.outputs[\"win_rate_b\"],\n",
    "        )\n",
    "    with dsl.Else():\n",
    "        hello_world().set_display_name(\"Trigger Tuning Job\") # Placeholder component for Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "756a20a3-83a9-4b3f-a1f7-0a67ee9b0f36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/757654702990/locations/us-central1/pipelineJobs/passage-gen-examplenibpw\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/757654702990/locations/us-central1/pipelineJobs/passage-gen-examplenibpw')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/passage-gen-examplenibpw?project=757654702990\n",
      "PipelineJob projects/757654702990/locations/us-central1/pipelineJobs/passage-gen-examplenibpw current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/757654702990/locations/us-central1/pipelineJobs/passage-gen-examplenibpw current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/757654702990/locations/us-central1/pipelineJobs/passage-gen-examplenibpw current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/757654702990/locations/us-central1/pipelineJobs/passage-gen-examplenibpw current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/757654702990/locations/us-central1/pipelineJobs/passage-gen-examplenibpw current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/757654702990/locations/us-central1/pipelineJobs/passage-gen-examplenibpw current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/757654702990/locations/us-central1/pipelineJobs/passage-gen-examplenibpw\n"
     ]
    }
   ],
   "source": [
    "template_uri = 'pipeline.yaml'\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=template_uri,\n",
    ")\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    job_id=DISPLAY_NAME,\n",
    "    display_name=\"pipeline-passage-gen\",\n",
    "    pipeline_root=os.path.join(BUCKET_URI, DISPLAY_NAME),\n",
    "    template_path=template_uri,\n",
    "    enable_caching=False,\n",
    ")\n",
    "job.run(sync=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29298704-c713-43e4-b686-846b6cbe5a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
